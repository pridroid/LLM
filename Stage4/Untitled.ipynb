{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a9852a45-5587-4758-9d55-084b63ce88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # load variables from .env into the environment\n",
    "\n",
    "# Access them like this\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "input_folder = Path(os.getenv(\"INPUT_FOLDER\"))\n",
    "output_folder = Path(os.getenv(\"OUTPUT_FOLDER\"))\n",
    "csv_output_folder = Path(os.getenv(\"CSV_FOLDER\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca745bf1-a8ea-4a72-a17c-b5be8595e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.llms import OpenAI\n",
    "import google.generativeai as genai\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d87607-6206-42ab-9a36-64fd40fd9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"WRITE_YOUR_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63858d7d-de87-40de-830b-cc9538370906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ff0c09-ea79-41f3-919d-8b97d0c3772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def chunk_text(text, model=\"gpt-3.5-turbo\", max_tokens=3000):\n",
    "    \"\"\"Splits text into chunks that fit within the token limit.\"\"\"\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    tokens = enc.encode(text)\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk = enc.decode(tokens[i : i + max_tokens])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d67e91e-3946-49ce-beb9-75958a36cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text,jd):\n",
    "    chunks = chunk_text(text, max_tokens=3000)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"text\",\"jd\"],\n",
    "            template=\"\"\"\n",
    "            Hey, act like a highly skilled ATS (Applicant Tracking System) with deep expertise in software engineering, \n",
    "            data science, and AI hiring. Evaluate the resume against the provided job description. The job market is \n",
    "            competitive, so provide the most insightful recommendations for improvement. \n",
    "    \n",
    "            Assign a **percentage match** between the resume and the JD, categorize missing keywords, \n",
    "            and highlight key strengths and potential concerns.\n",
    "    \n",
    "            Resume: {text}\n",
    "            Job Description: {jd}\n",
    "    \n",
    "            I want the response strictly in **JSON format** with the following structure:\n",
    "    \n",
    "            {{\n",
    "                \"Name\": \"<Candidate Name>\",\n",
    "                \"JD Match\": \"<% Match>\",\n",
    "                \"Missing Keywords\": {{\n",
    "                    \"Technical Skills\": [],\n",
    "                    \"Tools & Technologies\": [],\n",
    "                    \"Concepts & Methodologies\": []\n",
    "                }},\n",
    "                \"Profile Summary\": \"<Brief summary of the candidate if it is related to jd> \",\n",
    "                \"Projects\": [\n",
    "                    {{\n",
    "                        \"Project Name\": \"<Project Title>\",\n",
    "                        \"Relevance to JD\": \"<High/Medium/Low>\",\n",
    "                        \"Technologies Used\": [],\n",
    "                        \"Impact\": \"<Brief description of project outcomes>\"\n",
    "                    }}\n",
    "                ],\n",
    "                \"Certifications & Courses\": [\n",
    "                    \"<Relevant Certifications or Courses>\"\n",
    "                ],\n",
    "                \"Skills That Will Contribute to the Company\": [],\n",
    "                \"Soft Skills & Leadership Qualities\": [\n",
    "                    \"<Communication, Leadership, Problem-Solving, Teamwork, etc.>\"\n",
    "                ],\n",
    "                \"Industry Experience\": \"<Relevant experience in specific industries like Finance, Healthcare, etc.>\",\n",
    "                \"Culture Fit Assessment\": \"<High/Medium/Low> – Explanation of adaptability to company values\",\n",
    "                \"Potential Concerns\": [\"Any gaps, missing skills, or weaknesses\"],\n",
    "                \"Red Flags & Risk Analysis\": [\"Any major issues that could impact hiring decision\"],\n",
    "                \"Candidate’s Growth Potential\": \"<How much they can grow in the company>\",\n",
    "                \"Effort Needed by the Company\": \"<Low/Medium/High – Explanation>\"\n",
    "            }}\n",
    "            \"\"\"\n",
    "        \n",
    "        )\n",
    "\n",
    "    llm = OpenAI(temperature=0.7, max_tokens=1000,model=\"gpt-3.5-turbo\")  # Reduce max_tokens for output\n",
    "    prompt = prompt_template.format(text=chunk, jd=jd)\n",
    "    summary = llm(prompt)\n",
    "    summaries.append(summary)\n",
    "    \n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "289ec309-cf9e-47c8-bed3-5cbc2a6a13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd = \"\"\"\n",
    "Machine Learning Developer Job Description\n",
    "\n",
    "Job Title: Machine Learning Developer  \n",
    "Location: [Your Location or Remote]  \n",
    "Job Type: Full-time  \n",
    "\n",
    "About the Role:  \n",
    "We are seeking a **Machine Learning Developer** to design, develop, and deploy intelligent models that enhance our products and services. You will work on cutting-edge projects involving **data processing, model training, and deployment** to solve real-world problems.  \n",
    "\n",
    "Responsibilities:  \n",
    "- Develop, train, and optimize machine learning models for various applications.  \n",
    "- Process, clean, and analyze large datasets to extract meaningful insights.  \n",
    "- Design and implement scalable ML pipelines and deploy models in production.  \n",
    "- Collaborate with data scientists, engineers, and product teams to integrate ML solutions.  \n",
    "- Monitor and improve model performance using evaluation metrics.  \n",
    "- Stay updated with the latest advancements in ML, AI, and deep learning.  \n",
    "\n",
    "Required Skills & Qualifications:  \n",
    "- Bachelor's/Master’s degree in Computer Science, Data Science, or a related field.  \n",
    "- **Strong programming skills** in Python (NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch).  \n",
    "- **Experience with ML frameworks** like TensorFlow, PyTorch, or Keras.  \n",
    "- **Understanding of algorithms** like regression, classification, clustering, and deep learning.  \n",
    "- Experience with **data preprocessing and feature engineering**.  \n",
    "- Knowledge of **cloud platforms** (AWS, GCP, Azure) for model deployment.  \n",
    "- Experience with **APIs, Docker, Kubernetes** is a plus.  \n",
    "\n",
    "Preferred Qualifications:  \n",
    "- Experience with **NLP, computer vision, or reinforcement learning**.  \n",
    "- Knowledge of **big data technologies** like Spark, Hadoop.  \n",
    "- Experience with **MLOps, CI/CD pipelines for ML models**.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01089ce1-e027-47c8-99ab-2725cb42726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    \n",
    "    jd = \"\"\"\n",
    "Machine Learning Developer Job Description\n",
    "\n",
    "Job Title: Machine Learning Developer  \n",
    "Location: [Your Location or Remote]  \n",
    "Job Type: Full-time  \n",
    "\n",
    "About the Role:  \n",
    "We are seeking a **Machine Learning Developer** to design, develop, and deploy intelligent models that enhance our products and services. You will work on cutting-edge projects involving **data processing, model training, and deployment** to solve real-world problems.  \n",
    "\n",
    "Responsibilities:  \n",
    "- Develop, train, and optimize machine learning models for various applications.  \n",
    "- Process, clean, and analyze large datasets to extract meaningful insights.  \n",
    "- Design and implement scalable ML pipelines and deploy models in production.  \n",
    "- Collaborate with data scientists, engineers, and product teams to integrate ML solutions.  \n",
    "- Monitor and improve model performance using evaluation metrics.  \n",
    "- Stay updated with the latest advancements in ML, AI, and deep learning.  \n",
    "\n",
    "Required Skills & Qualifications:  \n",
    "- Bachelor's/Master’s degree in Computer Science, Data Science, or a related field.  \n",
    "- **Strong programming skills** in Python (NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch).  \n",
    "- **Experience with ML frameworks** like TensorFlow, PyTorch, or Keras.  \n",
    "- **Understanding of algorithms** like regression, classification, clustering, and deep learning.  \n",
    "- Experience with **data preprocessing and feature engineering**.  \n",
    "- Knowledge of **cloud platforms** (AWS, GCP, Azure) for model deployment.  \n",
    "- Experience with **APIs, Docker, Kubernetes** is a plus.  \n",
    "\n",
    "Preferred Qualifications:  \n",
    "- Experience with **NLP, computer vision, or reinforcement learning**.  \n",
    "- Knowledge of **big data technologies** like Spark, Hadoop.  \n",
    "- Experience with **MLOps, CI/CD pipelines for ML models**.  \n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "    summary = summarize_text(text,jd)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5384f3e1-bde9-4738-9ed6-ef98f87ec285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def summarize_multiple_pdfs(pdf_paths):\n",
    "    summaries = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        pdf_path = Path(pdf_path)  # Convert to Path object if it's a string\n",
    "        print(f\"Summarizing: {pdf_path.name}\")\n",
    "        summary = summarize_pdf(pdf_path)\n",
    "        summaries.append(f\"Summary for {pdf_path.name}:\\n{summary}\\n\")\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d3ab9f-8d1f-402e-9d84-b25fe72d568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pathlib import Path\n",
    "#import shutil\n",
    "\n",
    "# Paths\n",
    "#input_folder = Path(r'C:\\Users\\adity\\BE Project\\Final\\InputResume_Folder')\n",
    "#output_folder = Path(r'C:\\Users\\adity\\BE Project\\Final\\OutputResume_Folder')\n",
    "\n",
    "# Ensure output folder exists\n",
    "#output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get all PDFs\n",
    "#pdf_paths = list(input_folder.glob('*.pdf'))\n",
    "\n",
    "# Process each one\n",
    "#for pdf_path in pdf_paths:\n",
    " #   filename = pdf_path.name  # Just the file name, like \"resume01.pdf\"\n",
    "  #  print(f\"Processing: {filename}\")\n",
    "    \n",
    "   # summaries = summarize_multiple_pdfs([str(pdf_path)])\n",
    "    #for summary in summaries:\n",
    "     #   print(summary)\n",
    "    \n",
    "    # Move the file\n",
    "    #shutil.move(str(pdf_path), str(output_folder / filename))\n",
    "    #print(f\"Moved {filename} to OutputResume_Folder\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f038c691-2006-4eeb-813b-0629d71747de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def timestamp():\n",
    "    return datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "\n",
    "# === Start of the log ===\n",
    "with open(log_path, \"w\") as f:\n",
    "    f.write(f\"{timestamp()} BE Project — Resume Parsing Started\\n\")\n",
    "\n",
    "def write_log(filename, status, jd_match=None, error=None, details=None, final=False):\n",
    "    with open(log_path, \"a\") as f:\n",
    "        if status == \"START\":\n",
    "            f.write(f\"{timestamp()} Parsing Started: {filename}\\n\")\n",
    "        elif status == \"DETAILS\":\n",
    "            f.write(f\"{timestamp()} Resume Summary: {details}\\n\")\n",
    "        elif status == \"END\":\n",
    "            f.write(f\"{timestamp()} Parsing Completed: {filename}\\n\")\n",
    "        elif status == \"FAILED\":\n",
    "            f.write(f\"{timestamp()} Failed to Parse: {filename} | Error: {error}\\n\")\n",
    "        elif status == \"FINAL\" or final:\n",
    "            f.write(f\"{timestamp()} All Resumes Processed — Parsing Ended\\n\")\n",
    "\n",
    "\n",
    "# === Log Folder and File Path ===\n",
    "log_folder = Path(\"logs\")\n",
    "log_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_path = log_folder / f\"log_{timestamp_str}.txt\"\n",
    "\n",
    "def write_log(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(f\"[{timestamp}] {message}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84fdde8-30b5-477a-9b28-c309fa837f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d545100b-7ab5-4bdb-90e2-7257edc55ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Loaded JD from: jd.txt\n",
      "Processing: resume_15.pdf\n",
      "Moved resume_15.pdf to OutputResume_Folder\n",
      "\n",
      "✅ Data appended and saved to: C:\\Users\\adity\\BE Project\\Final\\Resume_Parsed_CSVs\\resume_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# === Configuration ===\n",
    "jd_folder = Path(\"JD_Folder\")\n",
    "import openai\n",
    "openai.api_key = \"your_openai_api_key_here\"\n",
    "\n",
    "input_folder = Path(r'InputResume_Folder')\n",
    "output_folder = Path(r'OutputResume_Folder')\n",
    "csv_output_folder = Path(\"Resume_Parsed_CSVs\")\n",
    "csv_path = csv_output_folder / \"resume_summary.csv\"\n",
    "\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "csv_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === PDF Text Extraction ===\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    for page in reader.pages:\n",
    "        if page.extract_text():\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# === Text Chunking for API limit ===\n",
    "def chunk_text(text, max_tokens=3000):\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_tokens:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# === Summarization with OpenAI ===\n",
    "def summarize_pdf(pdf_path,jd):\n",
    "    resume_text = extract_text_from_pdf(pdf_path)\n",
    "   # jd = \"Software Developer Job Description\"  # Replace with actual JD\n",
    "    chunks = chunk_text(resume_text)\n",
    "\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"text\", \"jd\"],\n",
    "            template=\"\"\"\n",
    "You are an AI assistant. Summarize the relevant resume details below in relation to the following job description.\n",
    "\n",
    "Resume Chunk:\n",
    "\\\"\\\"\\\"\n",
    "{text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Job Description:\n",
    "\\\"\\\"\\\"\n",
    "{jd}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Extract the key points in concise bullet form, JSON-style if possible. Do NOT return full JSON yet.\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        llm = ChatOpenAI(temperature=0.7, max_tokens=1000, model=\"gpt-3.5-turbo\")\n",
    "        prompt = prompt_template.format(text=chunk, jd=jd)\n",
    "        partial_summary = llm.predict(prompt)\n",
    "        summaries.append(partial_summary)\n",
    "\n",
    "    # === Final prompt: Ask GPT to merge partial summaries into final JSON ===\n",
    "    merge_prompt = f\"\"\"\n",
    "You are an expert resume evaluator.\n",
    "\n",
    "Below are summaries of different chunks of a resume, all based on the same candidate:\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "{''.join(summaries)}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Job Description:\n",
    "\\\"\\\"\\\"\n",
    "{jd}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Using these summaries, return a clean and complete JSON output with this structure:\n",
    "\n",
    "{{\n",
    "    \"Name\": \"<Candidate Name>\",\n",
    "    \"JD Match\": \"<% Match>\",\n",
    "    \"Missing Keywords\": {{\n",
    "        \"Technical Skills\": [],\n",
    "        \"Tools & Technologies\": [],\n",
    "        \"Concepts & Methodologies\": []\n",
    "    }},\n",
    "    \"Top Matching Keywords\": [],\n",
    "    \"Profile Summary\": \"<Brief summary related to JD>\",\n",
    "    \"Projects\": [\n",
    "        {{\n",
    "            \"Project Name\": \"<Title>\",\n",
    "            \"Relevance to JD\": \"<High/Medium/Low>\",\n",
    "            \"Technologies Used\": [],\n",
    "            \"Impact\": \"<Project outcomes>\"\n",
    "        }}\n",
    "    ],\n",
    "    \"Certifications & Courses\": [\"<Relevant Certifications>\"],\n",
    "    \"Skills That Will Contribute to the Company\": [],\n",
    "    \"Soft Skills & Leadership Qualities\": [\"<Communication, Leadership, etc.>\"],\n",
    "    \"Industry Experience\": \"<Relevant industries like Finance, Healthcare>\",\n",
    "    \"Culture Fit Assessment\": \"<High/Medium/Low – Explanation>\",\n",
    "    \"Potential Concerns\": [\"<Gaps, missing skills, weaknesses>\"],\n",
    "    \"Red Flags & Risk Analysis\": [\"<Major issues>\"],\n",
    "    \"Candidate’s Growth Potential\": \"<How much they can grow in the company>\",\n",
    "    \"Effort Needed by the Company\": \"<Low/Medium/High – Explanation>\",\n",
    "\n",
    "    \"Resume Strength Score\": \"<Numeric score between 0.0 and 10.0 in decimal format like 7.5>\",\n",
    "    \"Relevant Experience (yrs)\": \"<Years of directly relevant experience>\",\n",
    "    \"Employment Gaps Detected\": true,\n",
    "    \"Relevant Projects Count\": 0,\n",
    "    \"Resume Format Quality\": \"<Good/Average/Poor>\",\n",
    "    \"Candidate Type\": \"<Junior/Mid-Level/Senior>\",\n",
    "    \"HR Notes\": \"<Any special observations for HR>\",\n",
    "}}\n",
    "Return only the JSON.\n",
    "\"\"\"\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0.3, max_tokens=1500,model=\"gpt-3.5-turbo\")\n",
    "    final_response = llm.predict(merge_prompt)\n",
    "\n",
    "    try:\n",
    "        structured_data = json.loads(final_response)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"⚠️ Failed to parse JSON for {pdf_path.name}\")\n",
    "        structured_data = {}\n",
    "\n",
    "    return structured_data\n",
    "# === Load JD Dynamically ===\n",
    "jd_files = list(jd_folder.glob(\"*.txt\"))\n",
    "if not jd_files:\n",
    "    print(\"❌ No JD file found. Please add a .txt JD in JD_Folder.\")\n",
    "    exit()\n",
    "\n",
    "latest_jd_file = max(jd_files, key=lambda f: f.stat().st_mtime)\n",
    "with open(latest_jd_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    jd = f.read()\n",
    "print(f\"📄 Loaded JD from: {latest_jd_file.name}\")\n",
    "\n",
    "# === Main Process ===\n",
    "#write_log(\"LOG\", \"HEADER\")\n",
    "pdf_paths = list(input_folder.glob('*.pdf'))\n",
    "\n",
    "\n",
    "# ✅ Check if folder is empty\n",
    "if not pdf_paths:\n",
    "    print(\"📭 No resumes found in the input folder. Nothing to process.\")\n",
    "    exit()\n",
    "\n",
    "all_data = []\n",
    "\n",
    "write_log(\"BE Project Resume Parsing Start\")\n",
    "for pdf_path in pdf_paths:\n",
    "    write_log(f\"Parsing started: {pdf_path.name}\")\n",
    "    filename = pdf_path.name\n",
    "    print(f\"Processing: {filename}\")\n",
    " #   write_log(filename, \"START\")\n",
    "\n",
    "    try:\n",
    "        parsed_data = summarize_pdf(pdf_path,jd)\n",
    "        parsed_data[\"resume_name\"] = filename\n",
    "        all_data.append(parsed_data)\n",
    "        write_log(f\"Details: {json.dumps(parsed_data, indent=2)}\")\n",
    "        write_log(f\"Parsing completed: {pdf_path.name}\")\n",
    "        summary_line = f\"Name: {parsed_data.get('Name', 'N/A')} | JD Match: {parsed_data.get('JD Match', 'N/A')} | Resume Strength Score: {parsed_data.get('Resume Strength Score', 'N/A')}\"\n",
    "      #  write_log(filename, \"DETAILS\", details=summary_line)\n",
    "\n",
    "       # write_log(filename, \"END\")\n",
    "    except Exception as e:\n",
    "        #write_log(filename, \"FAILED\", error=str(e))\n",
    "        continue\n",
    "\n",
    "        # Extract JD Match % if available\n",
    "        jd_match = parsed_data.get(\"JD Match\", None)\n",
    "\n",
    "     #   write_log(filename, \"✅ Parsed\", jd_match=jd_match)\n",
    "    except Exception as e:\n",
    "      #  write_log(filename, \"❌ Failed\", error=str(e))\n",
    "        continue\n",
    "\n",
    "    shutil.move(str(pdf_path), str(output_folder / filename))\n",
    "    print(f\"Moved {filename} to OutputResume_Folder\\n\")\n",
    "\n",
    "\n",
    "# === Save to CSV (Append Mode) ===\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "try:\n",
    "    if csv_path.exists() and os.path.getsize(csv_path) > 0:\n",
    "        existing_df = pd.read_csv(csv_path)\n",
    "        combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = df\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"⚠️ Existing CSV is empty or corrupt. Starting fresh.\")\n",
    "    combined_df = df\n",
    "\n",
    "# ✅ Save result to CSV\n",
    "combined_df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ Data appended and saved to: {csv_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa22c1f0-8fa6-42fe-ad70-046ed91edd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resume CSV sorted by 'Resume Strength Score' and updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = \"Resume_Parsed_CSVs/resume_summary.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert 'Resume Strength Score' to numeric (in case it's stored as string like \"7/10\")\n",
    "df[\"Resume Strength Score\"] = pd.to_numeric(df[\"Resume Strength Score\"], errors=\"coerce\")\n",
    "\n",
    "# Sort by Resume Strength Score in descending order\n",
    "df_sorted = df.sort_values(by=\"Resume Strength Score\", ascending=False)\n",
    "\n",
    "# Save the sorted DataFrame back to CSV\n",
    "df_sorted.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"✅ Resume CSV sorted by 'Resume Strength Score' and updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe7fdf-416b-486f-b42a-ac5453c14f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e197e209-b4ff-451c-a434-a54b24d76c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
